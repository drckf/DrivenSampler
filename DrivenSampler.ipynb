{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'png')\n",
    "plt.rcParams['savefig.dpi'] = 75\n",
    "\n",
    "plt.rcParams['figure.autolayout'] = False\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "plt.rcParams['lines.markersize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.serif'] = \"cm\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training Generative Neural Networks Out-of-Equilibrium\n",
    "\n",
    "### Authors:\n",
    "\n",
    "Charles K. Fisher; charleskennethfisher@gmail.com,\n",
    "Jonathan R. Walsh; jrwalsh1@gmail.com\n",
    "Austin Huang; austinh@alum.mit.edu\n",
    "Aaron M. Smith; geminatea@gmail.com\n",
    "\n",
    "### Abstract:\n",
    "\n",
    "Restricted Boltzmann Machines (RBMs) are a class of neural network that can be used to approximate arbitrary probability distributions. Unlike feed-forward neural networks, RBMs are stochastic, which makes them perform well as trainable generative models.  Training an RBM through stochastic gradient descent is difficult, however, because it is necessary to draw a sample from the model distribution for each gradient evaluation. \n",
    "\n",
    "This notebook describes an approach to sampling from an RBM by coupling the distribution to a slowly varying auxiliary variable. The non-equilibrium sampler allows the system to move around energy barriers, which results in better mixing. This simple modification leads to large improvements in training -- especially for RBMs with Gaussian visible units -- with minimal computational cost compared to standard samplingmethods. \n",
    "\n",
    "### About this project:\n",
    "\n",
    "This notebook is an experiment in *open science*. It will provide all of the background information, data sets, and code required for a traditional scientific publication -- but in an open-source format that anybody can access (or even contribute to). \n",
    "\n",
    "This project is connected to [paysage](https://github.com/drckf/paysage): a Python library for machine learning with Boltzmann machines. The non-equilibrium sampler discussed in this notebook is implemented in paysage as the class `DrivenSequentialMC`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Recent progress in machine learning and artificial intelligence has been driven by a renaissance in research on artificial neural networks. Deep learning methods have been especially powerful tools for large-scale supervised learning problems. In fact, one could go so far as to claim that there is now a well-established recipe for success on supervised learning problems: step 1 -- collect a very large dataset with reliable labels (for example, a billion images labeled as \"cat\" or \"not cat\"), step 2 -- apply some noise or nuissance transformations to the data that you want your model to be insensitive to, step 3 -- use stochastic gradient descent to train a very large multi-layer neural network on these data. Unfortunately, there are many problems that cannot be solved using this three step recipe, usually because of a failure at step 1. If there is little or no data with reliable labels then we have to throw away the cookbook and try something else.\n",
    "\n",
    "Unsupervised learning is the task of learning from data without labels. In the unsupervised context, \"learning\" refers to developing an understanding of the *process that generates the data*, or to constructing a simpler (e.g., sparse or low-dimensional) *representation* of the data. A Restricted Boltzmann Machine, or RBM, is a type of neural network designed for this task.  \n",
    "\n",
    "Training an RBM on an unsupervised problem is not as easy as training a neural networks for a supervised problem. Neural networks used for supervised problems are deterministic -- they assign a unique label to every image. Moreover, the gradient of the objective function can be calculated very easily. By contrast, RBMs are stochastic; they describe probablity distributions instead of input-output functions. As a result, computing the gradients exactly is so costly that it is effectively impossible. Therefore, the ability to train RBMs efficiently hinges on the development of fast and accurate approximates to the gradients of the objective function.\n",
    "\n",
    "\n",
    "### Restricted Boltzmann Machines\n",
    "\n",
    "An RBM is a type of energy based model defined through an energy function. Let $v_i$ for $i = 1, \\ldots, N$ denote the units of the visible layer and $h_{\\mu}$ for $\\mu = 1, \\ldots, M$ denote the units of the hidden layer. The energy function of a general RBM is:\n",
    "\n",
    "\\begin{equation}\n",
    "H(\\boldsymbol{v}, \\boldsymbol{h}) = -\\sum_i f_i(v_i) - \\sum_{\\mu} f_{\\mu}(h_{\\mu}) \n",
    "- \\sum_{i \\mu} W_{i \\mu} g_i(v_i) g_{\\mu}(h_{\\mu})\n",
    "\\end{equation}\n",
    "\n",
    "Here, $f_i(\\cdot)$ and $g_i(\\cdot)$ are functions defined for each visible unit, and $f_{\\mu}(\\cdot)$ and $g_{\\mu}(\\cdot)$ are functions defined for each hidden unit. The joint probability distribution of the visible and hidden units is obtained by analogy with Boltzmann's distribution is physics:\n",
    "\n",
    "\\begin{equation}\n",
    "p_{RBM}(\\boldsymbol{v}, \\boldsymbol{h}) = Z^{-1} e^{-H(\\boldsymbol{v}, \\boldsymbol{h}) }\n",
    "\\end{equation}\n",
    "\n",
    "where $Z = \\int d \\boldsymbol{v} d \\boldsymbol{h} e^{-H(\\boldsymbol{v}, \\boldsymbol{h}) }$ is the normalizing constant of the distribution (also called the partition function). \n",
    "\n",
    "\n",
    "Training an RBM involves optimizing the parameters of the model so that the marginal distribution of the visible units is approximately equal to the data distribution:\n",
    "\\begin{equation}\n",
    "p_{data}(\\boldsymbol{v})\\approx p_{RBM}(\\boldsymbol{v}) = Z^{-1} \\int d \\boldsymbol{h} e^{-H(\\boldsymbol{v}, \\boldsymbol{h}) }\n",
    "\\end{equation}\n",
    "\n",
    "We formulate the training problem as an optimization problem aimed at minimizing a measure of difference between $p_{data}(\\boldsymbol{v})$ and $p_{RBM}(\\boldsymbol{v})$ called the Kullback-Leibler divergence:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(p_{data}(\\boldsymbol{v}) || p_{RBM}(\\boldsymbol{v})) = -E_{data}[\\log p_{RBM}(\\boldsymbol{v}) ] + \\text{constant}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "import paysage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Acknowledgements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
